{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Python\n",
    "\n",
    "Hopefully now you are feeling a bit more comfortable with Python, Kaggle, and modeling. \n",
    "\n",
    "The next kaggle project:\n",
    "\n",
    "https://www.kaggle.com/c/costa-rican-household-poverty-prediction\n",
    "\n",
    "### Grading\n",
    "\n",
    "This homework is due **March 14, 2019 by 4:00 pm Utah time.** By that time, you need to have committed all your code to your github and submitted a link to your work to the TA. We can see on your Github account when you last committed code. :)\n",
    "\n",
    "Rubric:\n",
    "\n",
    "* Code Quality - 10%\n",
    "* Storytelling - 10%\n",
    "* Result on Kaggle - 5%\n",
    "* Describing, Cleaning, and Visualizing data - 25%\n",
    "* Modeling - 50%\n",
    "\n",
    "More specifically, for modeling we will look for: \n",
    "\n",
    "* Model Selection: Did you try multiple models? Why did you choose these models? How do they work? What are they assumptions? And how did you test/account for them? How did you select hyper-parameters?\n",
    "* Model evaluation: Did you evaluate your model on multiple metrics? Where does your model do well? Where could it be improved? How are the metrics different?\n",
    "* Model interpretation: What do the model results tell you? Which variables are important? High bias or variance and how did you / could you fix this? How confident are you in your results? \n",
    "* Model usefulness: Do you think your final model was useful? If so, how would you recommend using it? Convince us, that if we were a company, we would feel comfortable using your model with our users. Think about edge cases as well - are there certain areas that the model performs poorly on? Best on? How would you handle these cases, if say Zillow wanted to leverage your model realizing that bad recommendations on sale prices would hurt customer trust and your brand. This section also falls into the storytelling aspect of the grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig', 'v18q',\n",
       "       'v18q1', 'r4h1',\n",
       "       ...\n",
       "       'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin',\n",
       "       'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq', 'Target'],\n",
       "      dtype='object', length=143)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "data_train = pd.read_csv('train_costarica.csv')\n",
    "data_test = pd.read_csv('test_costarica.csv')\n",
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = data_train['Id']\n",
    "test_ID = data_test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "data_train.drop(\"Id\", axis = 1, inplace = True)\n",
    "data_test.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>r4h2</th>\n",
       "      <th>...</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>agesq</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>1849</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1849</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>4489</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>8464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>8464</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>289</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>1369</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  r4h1  r4h2  \\\n",
       "0  190000.0       0      3       0     1       1     0    NaN     0     1   \n",
       "1  135000.0       0      4       0     1       1     1    1.0     0     1   \n",
       "2       NaN       0      8       0     1       1     0    NaN     0     0   \n",
       "3  180000.0       0      5       0     1       1     1    1.0     0     2   \n",
       "4  180000.0       0      5       0     1       1     1    1.0     0     2   \n",
       "\n",
       "    ...    SQBescolari  SQBage  SQBhogar_total  SQBedjefe  SQBhogar_nin  \\\n",
       "0   ...            100    1849               1        100             0   \n",
       "1   ...            144    4489               1        144             0   \n",
       "2   ...            121    8464               1          0             0   \n",
       "3   ...             81     289              16        121             4   \n",
       "4   ...            121    1369              16        121             4   \n",
       "\n",
       "   SQBovercrowding  SQBdependency  SQBmeaned  agesq  Target  \n",
       "0         1.000000            0.0      100.0   1849       4  \n",
       "1         1.000000           64.0      144.0   4489       4  \n",
       "2         0.250000           64.0      121.0   8464       4  \n",
       "3         1.777778            1.0      121.0    289       4  \n",
       "4         1.777778            1.0      121.0   1369       4  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rez_esc</th>\n",
       "      <td>7928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v18q1</th>\n",
       "      <td>7342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v2a1</th>\n",
       "      <td>6860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQBmeaned</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meaneduc</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "rez_esc    7928\n",
       "v18q1      7342\n",
       "v2a1       6860\n",
       "SQBmeaned     5\n",
       "meaneduc      5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the top columns with missing values\n",
    "missmap = data_train.isnull().sum().to_frame()\n",
    "missmap = missmap.sort_values(0, ascending = False)\n",
    "missmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering function I found online\n",
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('bed_density', 'bedrooms', 'rooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                 ('male_over_female', 'r4h3', 'r4m3'),\n",
    "                 ('man12plus_over_women12plus', 'r4h2', 'r4m2'),\n",
    "                 ('pesioner_over_working', 'hogar_mayor', 'hogar_adul'),\n",
    "                 ('children_over_working', 'hogar_nin', 'hogar_adul'),\n",
    "                 ('education_fraction', 'escolari', 'age')\n",
    "                 #('', '', ''),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('non_bedrooms', 'rooms', 'bedrooms'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean', 'count'],\n",
    "                'escolari': ['min', 'max', 'mean', 'std'],\n",
    "                'fe_education_fraction': ['min', 'max', 'mean', 'std']\n",
    "               }\n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean']\n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    # do something advanced above...\n",
    "    \n",
    "    # Drop SQB variables, as they are just squres of other vars \n",
    "    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n",
    "    # Drop repeated columns\n",
    "    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function replaces null values with -1 and 0 values depending on variable\n",
    "def process_df(df_):\n",
    "    for f in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n",
    "        df_[f] = df_[f].fillna(0)\n",
    "    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n",
    "    return (df_)\n",
    "\n",
    "#Run function on training and testing data\n",
    "data_train = process_df(data_train)\n",
    "data_test = process_df(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save X and Y and drop the Id from x\n",
    "#y = train['Target']\n",
    "#X = train.drop(['Target'], axis=1).copy()\n",
    "\n",
    "#combine the training and testing data\n",
    "all_data = pd.concat((data_train.drop(['Target'], axis=1), data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dummy variables for all the categorical features\n",
    "all_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now create our training and testing data again\n",
    "train = all_data[:data_train.shape[0]]\n",
    "test = all_data[data_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save X and Y and drop the Id from x\n",
    "y = data_train['Target']\n",
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9557.000000\n",
       "mean        3.302292\n",
       "std         1.009565\n",
       "min         1.000000\n",
       "25%         3.000000\n",
       "50%         4.000000\n",
       "75%         4.000000\n",
       "max         4.000000\n",
       "Name: Target, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get an overview of the data\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.627394\n",
       "2    0.167103\n",
       "3    0.126504\n",
       "1    0.079000\n",
       "Name: Target, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the distribution of y\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,  755., 1597., 1209., 5996.,    0.]),\n",
       " array([0, 1, 2, 3, 4, 5, 6]),\n",
       " <a list of 6 Patch objects>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here we can visually see the distribution of y\n",
    "plt.hist(y, bins=[0,1,2,3,4,5,6], color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the 20 most correlated variables\n",
    "#abs(train.corr()['Target']).sort_values(ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### None of the variables were highly correlated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] n_estimators=100 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. n_estimators=100, total=   4.5s\n",
      "[CV] n_estimators=100 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. n_estimators=100, total=   1.6s\n",
      "[CV] n_estimators=500 ................................................\n",
      "[CV] ................................. n_estimators=500, total=   2.3s\n",
      "[CV] n_estimators=500 ................................................\n",
      "[CV] ................................. n_estimators=500, total=   2.2s\n",
      "[CV] n_estimators=1000 ...............................................\n",
      "[CV] ................................ n_estimators=1000, total=   3.2s\n",
      "[CV] n_estimators=1000 ...............................................\n",
      "[CV] ................................ n_estimators=1000, total=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   20.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=3, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators='warn', n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 500, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will create a random forest\n",
    "param_dictionary = {\"n_estimators\": [100, 500, 1000]}\n",
    "RFC = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=3)\n",
    "gs = GridSearchCV(RFC, param_dictionary, n_jobs=1, verbose=2, cv=2)\n",
    "gs.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I was curious to see which was best but since it is random each time the \"best\" parameter is different.\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.31      0.64      0.42       591\n",
      "           2       0.43      0.32      0.37      1286\n",
      "           3       0.33      0.33      0.33       955\n",
      "           4       0.83      0.77      0.80      4813\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      7645\n",
      "   macro avg       0.48      0.52      0.48      7645\n",
      "weighted avg       0.66      0.63      0.64      7645\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#get the predictions and training scores for x_tr\n",
    "train_predictions = gs.predict(x_tr)\n",
    "cr = classification_report(y_tr, train_predictions)\n",
    "print(\"Training Scores:\")\n",
    "print(cr)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.63      0.44       164\n",
      "           2       0.43      0.31      0.36       311\n",
      "           3       0.31      0.27      0.29       254\n",
      "           4       0.82      0.79      0.81      1183\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1912\n",
      "   macro avg       0.47      0.50      0.47      1912\n",
      "weighted avg       0.65      0.63      0.63      1912\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#get the predictions and training scores for x_val\n",
    "val_predictions = gs.predict(x_val)\n",
    "cr = classification_report(y_val, val_predictions)\n",
    "print('Validation Scores:')\n",
    "print(cr)\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meaneduc', 0.02563936373134907),\n",
       " ('eviv3', 0.021097471270354645),\n",
       " ('SQBmeaned', 0.019948170301993827),\n",
       " ('instlevel8', 0.01936029535000196),\n",
       " ('SQBedjefe', 0.01840844329830974),\n",
       " ('etecho3', 0.018111453036661018),\n",
       " ('SQBhogar_nin', 0.018013393080849434),\n",
       " ('epared3', 0.017704978405413904),\n",
       " ('SQBescolari', 0.017474754292521193),\n",
       " ('cielorazo', 0.017470463733033202)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at feature importances for our model after training it\n",
    "feat_imports = sorted(list(zip(X.columns, gs.best_estimator_.feature_importances_)), key=lambda x:x[1], reverse=True)\n",
    "feat_imports[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=5, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on all of our data\n",
    "clf = RandomForestClassifier(n_jobs=-1, max_depth=5, n_estimators=1000, class_weight='balanced', verbose=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "#get test predictions\n",
    "test_predictions = clf.predict(pd.get_dummies(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "#Error Analysis\n",
    "probabilities = clf.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create submission\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['Target'] = test_predictions\n",
    "sub.to_csv('costarica1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I enjoyed this competition but honestly I wish I would have had more time to work on it.  I have been slammed with midterms the past two weeks and to top it off I got crazy sick this week.  So I have not been able to put in the time or energy I would've like to into this assignment.  If I had the time (or the health) I would've spent much more time trying different models and trying to find the one that best predicted the data.  So instead after trying a few different models, I decided to use a Random Forest because we had talked about it in class and how it is a good \"go-to\" model.  Since I had never used one before, I thought this would be the perfect assignment to use a Random Forest on.  I really enjoyed practicing what we learned in class.  I have a much better understanding of Random Forests and Grid Search now.  \n",
    "\n",
    "##### Honestly, this model did not perform super well.  I only had a score of 0.37481 which wasn't great.  There was probably a lot more I could've done to better prepare the data and to better fit the model.  \n",
    "\n",
    "##### Like I've said, I don't think this model was extremely useful.  There is a lot more I would want to do before offering my model to a company to use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
